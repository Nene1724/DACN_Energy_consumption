# H∆∞·ªõng D·∫´n Hu·∫•n Luy·ªán L·∫°i Energy Prediction Models

## üìã T·ªïng Quan

T√≠nh nƒÉng Energy Prediction hi·ªán c√≥ **MAPE 15-19%** (t·ªët h∆°n b·∫°n nghƒ©!), nh∆∞ng c√≥ th·ªÉ c·∫£i thi·ªán th√™m.

### Hi·ªán tr·∫°ng:
- ‚úÖ **Jetson Nano**: MAPE 18.69%, R¬≤=0.86 (247 models)
- ‚úÖ **Raspberry Pi 5**: MAPE 15.88%, R¬≤=0.95 (27 models)
- ‚ö†Ô∏è **V·∫•n ƒë·ªÅ**: RPi5 c√≥ qu√° √≠t data (27 models), Jetson c√≥ th·ªÉ c·∫£i thi·ªán th√™m

---

## üöÄ B∆∞·ªõc 1: Chu·∫©n B·ªã M√¥i Tr∆∞·ªùng

### 1.1. Chuy·ªÉn v√†o th∆∞ m·ª•c d·ª± √°n
```powershell
cd D:\DACN_BACKUP\DACN_Energy_consumption\ml-controller
```

### 1.2. C√†i ƒë·∫∑t dependencies
```powershell
pip install -r requirements.txt

# Ho·∫∑c c√†i th·ªß c√¥ng:
pip install pandas numpy scikit-learn matplotlib seaborn jupyter
pip install xgboost lightgbm  # Optional: ƒë·ªÉ th·ª≠ thu·∫≠t to√°n m·ªõi
```

### 1.3. M·ªü Jupyter Notebook
```powershell
jupyter notebook notebooks/energy_prediction_model.ipynb
```

Ho·∫∑c m·ªü tr·ª±c ti·∫øp trong VS Code:
```powershell
code notebooks/energy_prediction_model.ipynb
```

---

## üìä B∆∞·ªõc 2: Thu Th·∫≠p Th√™m D·ªØ Li·ªáu (Khuy·∫øn ngh·ªã!)

### 2.1. Benchmark th√™m models tr√™n Raspberry Pi 5

**C·∫ßn:** TƒÉng t·ª´ 27 ‚Üí 80-100 models ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c

**C√°ch l√†m:**
1. Ch·∫°y benchmark framework tr√™n RPi5:
```bash
# Tr√™n Raspberry Pi 5
cd /path/to/benchmark
python benchmark_models.py --device rpi5 --models all
```

2. Export k·∫øt qu·∫£ ra CSV v·ªõi format:
```csv
model,params_m,gflops,gmacs,size_mb,latency_avg_s,throughput_iter_per_s,energy_avg_mwh
mobilenetv3_small_075,2.04,0.044,0.022,8.2,0.0234,42.7,8.5
efficientnet_b0,5.29,0.39,0.19,21.1,0.0567,17.6,15.2
...
```

3. Append v√†o file:
```powershell
# Merge v·ªõi file hi·ªán t·∫°i
# data/27_models_benchmark_rpi5.csv
```

### 2.2. (Optional) Benchmark th√™m models tr√™n Jetson Nano

Jetson ƒë√£ c√≥ 247 models (ƒë·ªß), nh∆∞ng n·∫øu mu·ªën th√™m:
```bash
# Tr√™n Jetson Nano
python benchmark_models.py --device jetson --models new_architectures
```

---

## üîß B∆∞·ªõc 3: Retrain Models

### 3.1. Run to√†n b·ªô Notebook

**Trong VS Code ho·∫∑c Jupyter:**
1. M·ªü [energy_prediction_model.ipynb](notebooks/energy_prediction_model.ipynb)
2. Click **"Run All"** ho·∫∑c `Ctrl+Shift+Enter` cho t·ª´ng cell
3. ƒê·ª£i kho·∫£ng 5-10 ph√∫t

### 3.2. Ki·ªÉm tra k·∫øt qu·∫£ training

Sau khi run, xem metrics:

```
=== Jetson Nano Model ===
Test MAPE: 18.69%  (target: < 15%)
Test R¬≤: 0.860      (target: > 0.90)

=== Raspberry Pi 5 Model ===
LOO MAPE: 15.88%   (target: < 12%)
LOO R¬≤: 0.946      (target: > 0.95)
```

### 3.3. Export models m·ªõi

Notebook s·∫Ω t·ª± ƒë·ªông l∆∞u models v√†o `artifacts/`:
```
‚úÖ Saved: artifacts/jetson_energy_model.pkl
‚úÖ Saved: artifacts/jetson_scaler.pkl
‚úÖ Saved: artifacts/rpi5_energy_model.pkl
‚úÖ Saved: artifacts/rpi5_scaler.pkl
‚úÖ Saved: artifacts/device_specific_metadata.json
```

---

## ‚ö° B∆∞·ªõc 4: C·∫£i Thi·ªán N√¢ng Cao

### 4.1. Th·ª≠ Thu·∫≠t To√°n Kh√°c (XGBoost, LightGBM)

**Th√™m cell m·ªõi v√†o notebook:**

```python
# Cell m·ªõi: Th·ª≠ XGBoost
import xgboost as xgb

# Train XGBoost cho Jetson
xgb_model = xgb.XGBRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    random_state=42
)
xgb_model.fit(X_train_scaled, y_train)
xgb_pred = xgb_model.predict(X_test_scaled)

# ƒê√°nh gi√°
from sklearn.metrics import mean_absolute_percentage_error, r2_score
xgb_mape = mean_absolute_percentage_error(y_test, xgb_pred) * 100
xgb_r2 = r2_score(y_test, xgb_pred)

print(f"XGBoost MAPE: {xgb_mape:.2f}%")
print(f"XGBoost R¬≤: {xgb_r2:.3f}")
```

```python
# Cell m·ªõi: Th·ª≠ LightGBM
import lightgbm as lgb

lgb_model = lgb.LGBMRegressor(
    n_estimators=200,
    max_depth=5,
    learning_rate=0.05,
    random_state=42
)
lgb_model.fit(X_train_scaled, y_train)
lgb_pred = lgb_model.predict(X_test_scaled)

lgb_mape = mean_absolute_percentage_error(y_test, lgb_pred) * 100
lgb_r2 = r2_score(y_test, lgb_pred)

print(f"LightGBM MAPE: {lgb_mape:.2f}%")
print(f"LightGBM R¬≤: {lgb_r2:.3f}")
```

### 4.2. Hyperparameter Tuning

**Th√™m cell tuning:**

```python
from sklearn.model_selection import GridSearchCV

# Grid search cho GradientBoostingRegressor
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'min_samples_split': [2, 5, 10],
    'subsample': [0.8, 0.9, 1.0]
}

grid_search = GridSearchCV(
    GradientBoostingRegressor(random_state=42),
    param_grid,
    cv=5,
    scoring='neg_mean_absolute_percentage_error',
    n_jobs=-1,
    verbose=2
)

print("üîç Searching for best hyperparameters...")
grid_search.fit(X_train_scaled, y_train)

print(f"‚úÖ Best params: {grid_search.best_params_}")
print(f"‚úÖ Best CV score: {-grid_search.best_score_:.2f}%")

# S·ª≠ d·ª•ng best model
best_model = grid_search.best_estimator_
```

### 4.3. Feature Engineering M·ªõi

**Th√™m features v√†o cell Feature Engineering:**

```python
# Th√™m v√†o section "Derived Features"

# 1. Architecture complexity
df['arch_complexity'] = df['params_m'] * df['gflops'] / (df['size_mb'] + 1e-6)

# 2. Efficiency score
df['efficiency_score'] = df['throughput_iter_per_s'] / (df['energy_avg_mwh'] + 1e-6)

# 3. Memory bandwidth requirement
df['memory_bandwidth'] = df['size_mb'] / (df['latency_avg_s'] + 1e-6)

# 4. FLOPs per second
df['flops_per_second'] = df['gflops'] * 1e9 * df['throughput_iter_per_s']

# 5. Energy per FLOP
df['energy_per_gflop'] = df['energy_avg_mwh'] / (df['gflops'] + 1e-6)

# Update feature list
feature_cols = [
    'params_m', 'gflops', 'gmacs', 'size_mb', 
    'latency_avg_s', 'throughput_iter_per_s',
    'params_per_gflop', 'gflops_per_mb', 'computational_density',
    'arch_complexity', 'efficiency_score', 'memory_bandwidth',
    'flops_per_second', 'energy_per_gflop'
]
```

### 4.4. Ensemble Models

**Th√™m cell ensemble:**

```python
# Ensemble: K·∫øt h·ª£p nhi·ªÅu models
from sklearn.ensemble import VotingRegressor

ensemble = VotingRegressor([
    ('gb', GradientBoostingRegressor(n_estimators=200, max_depth=5, learning_rate=0.05)),
    ('xgb', xgb.XGBRegressor(n_estimators=200, max_depth=5, learning_rate=0.05)),
    ('lgb', lgb.LGBMRegressor(n_estimators=200, max_depth=5, learning_rate=0.05))
])

ensemble.fit(X_train_scaled, y_train)
ensemble_pred = ensemble.predict(X_test_scaled)

ensemble_mape = mean_absolute_percentage_error(y_test, ensemble_pred) * 100
ensemble_r2 = r2_score(y_test, ensemble_pred)

print(f"Ensemble MAPE: {ensemble_mape:.2f}%")
print(f"Ensemble R¬≤: {ensemble_r2:.3f}")
```

---

## ‚úÖ B∆∞·ªõc 5: Test Models M·ªõi

### 5.1. Restart Flask server

```powershell
cd D:\DACN_BACKUP\DACN_Energy_consumption\ml-controller\python
python app.py
```

### 5.2. Test qua web dashboard

1. M·ªü http://localhost:5000
2. Ch·ªçn device (Jetson ho·∫∑c RPi5)
3. Ch·ªçn model t·ª´ popular list
4. Click **"Predict Energy"**
5. Xem k·∫øt qu·∫£ v√† MAPE m·ªõi

### 5.3. Test qua API

```powershell
# Test Jetson
Invoke-RestMethod -Uri "http://localhost:5000/api/predict" -Method POST -ContentType "application/json" -Body '{
  "payloads": [{
    "device_type": "jetson_nano",
    "model": "mobilenetv3_small_075",
    "params_m": 2.04,
    "gflops": 0.044,
    "gmacs": 0.022,
    "size_mb": 8.2,
    "latency_avg_s": 0.0234,
    "throughput_iter_per_s": 42.7
  }]
}'
```

---

## üìà B∆∞·ªõc 6: So S√°nh K·∫øt Qu·∫£

### 6.1. Metrics c≈© (hi·ªán t·∫°i):
```
Jetson: MAPE 18.69%, R¬≤ 0.860
RPi5:   MAPE 15.88%, R¬≤ 0.946
```

### 6.2. Metrics m·ªõi (sau khi retrain):
```
# Ghi l·∫°i k·∫øt qu·∫£ sau khi ch·∫°y xong notebook
Jetson: MAPE ___%, R¬≤ ___
RPi5:   MAPE ___%, R¬≤ ___
```

### 6.3. Target benchmarks:
```
‚úÖ EXCELLENT: MAPE < 12%, R¬≤ > 0.95
‚úÖ GOOD:      MAPE < 18%, R¬≤ > 0.90
‚ö†Ô∏è ACCEPTABLE: MAPE < 25%, R¬≤ > 0.80
```

---

## üéØ Checklist C·∫£i Thi·ªán

- [ ] Thu th·∫≠p th√™m 50-70 models cho RPi5 (quan tr·ªçng nh·∫•t!)
- [ ] Run l·∫°i to√†n b·ªô notebook hi·ªán t·∫°i
- [ ] Th·ª≠ XGBoost
- [ ] Th·ª≠ LightGBM
- [ ] Hyperparameter tuning v·ªõi GridSearchCV
- [ ] Th√™m features m·ªõi
- [ ] Ensemble models
- [ ] Test v√† so s√°nh k·∫øt qu·∫£
- [ ] Update metadata file
- [ ] Document improvements

---

## üîç Troubleshooting

### L·ªói: "ModuleNotFoundError: No module named 'sklearn'"
```powershell
pip install scikit-learn
```

### L·ªói: "FileNotFoundError: data/247_models_benchmark_jetson.csv"
```powershell
# Ki·ªÉm tra path
cd D:\DACN_BACKUP\DACN_Energy_consumption\ml-controller
ls data\
```

### Notebook ch·∫°y ch·∫≠m
- Gi·∫£m `n_estimators` xu·ªëng 100-150
- T·∫Øt GridSearchCV (ch·∫°y ri√™ng sau)
- S·ª≠ d·ª•ng subset data ƒë·ªÉ test nhanh

### MAPE v·∫´n cao sau retrain
- **Nguy√™n nh√¢n 1**: Data quality (outliers, missing values)
- **Nguy√™n nh√¢n 2**: Feature engineering ch∆∞a t·ªët
- **Nguy√™n nh√¢n 3**: Qu√° √≠t data (ƒë·∫∑c bi·ªát RPi5)
- **Gi·∫£i ph√°p**: Thu th·∫≠p th√™m data l√† quan tr·ªçng nh·∫•t!

---

## üìö T√†i Li·ªáu Tham Kh·∫£o

- [Notebook Training](notebooks/energy_prediction_model.ipynb)
- [Energy Predictor Service](python/energy_predictor_service.py)
- [Current Metadata](artifacts/device_specific_metadata.json)
- [User Guide](ENERGY_PREDICTION_USER_GUIDE.md)

---

**üéì Tips:**
1. **Data > Algorithm**: Thu th·∫≠p th√™m data hi·ªáu qu·∫£ h∆°n tune model
2. **Start Simple**: Ch·∫°y l·∫°i notebook hi·ªán t·∫°i tr∆∞·ªõc khi th·ª≠ advanced techniques
3. **Validate Carefully**: Lu√¥n test tr√™n unseen data
4. **Document Everything**: Ghi l·∫°i m·ªçi thay ƒë·ªïi v√† k·∫øt qu·∫£
